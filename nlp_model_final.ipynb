{"cells":[{"cell_type":"markdown","metadata":{"id":"QPnOdOwb-SUh"},"source":["model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hFObPWNAeIao","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660928644151,"user_tz":-540,"elapsed":4078,"user":{"displayName":"현시은","userId":"03817187635924431394"}},"outputId":"e1fc3b67-1fb2-4e01-de15-28cf7317bbc3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers==3.0.2 in /usr/local/lib/python3.7/dist-packages (3.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2022.6.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (1.21.6)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (4.64.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (0.0.53)\n","Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (0.1.97)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (3.8.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (21.3)\n","Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (0.8.1rc1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2.23.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.0.2) (3.0.9)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (2022.6.15)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (7.1.2)\n"]}],"source":["!pip install transformers==3.0.2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pDmsaQJ0rnyg"},"outputs":[],"source":["def mount_drive():  \n","    from google.colab import drive\n","    drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2942,"status":"ok","timestamp":1660928647085,"user":{"displayName":"현시은","userId":"03817187635924431394"},"user_tz":-540},"id":"sMTTH0AAiV9O","outputId":"a20bd595-490e-4a1e-d761-8cbb95031015"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["if __name__ == \"__main__\":\n","    mount_drive()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Tj07ECUMfN-"},"outputs":[],"source":["import torch\n","from transformers import get_linear_schedule_with_warmup, BertForSequenceClassification, BertConfig\n","from transformers import get_cosine_schedule_with_warmup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"07CmMoztqKZJ"},"outputs":[],"source":["from torch import nn\n","from transformers import BertModel, BertConfig\n","\n","class BertClassifier(nn.Module):\n","\n","    # BertClassifier는 초대용량 AI 모델인 BertModel을 파인튜닝한 모델이다.\n","    # 구체적으로, BertModel 위에 linear한 층 하나를 추가해 놓은 모델이다.\n","    # Bert -> Linear -> Softmax의 순서로 연산이 이루어진다.\n","    # 우리가 Loss를 구하는데 사용되는 CrossEntropyLoss의 경우, softmax를 자동으로 포함하여 연산해준다.\n","    # 고로, BertClassifier 클래스 내의 forward 함수에, softmax는 포함시키지 않는다.\n","    # 단, 학습의 용도가 아니라 테스트나 실제 시연을 할 때는 forward 결과물에 softmax를 적용시켜줘야 한다.\n","\n","    def __init__(self, num_labels=7):\n","\n","        super(BertClassifier, self).__init__()\n","\n","        model_config = BertConfig.from_pretrained('monologg/kobert')\n","        \n","        # model_config와 from_pretrained 함수를 활용해서 bert 모델을 선언해주자.\n","        # self.bert = ~~ 꼴로 작성해주자.\n","        ## 여기에 코드 작성\n","        self.bert = BertModel(model_config)\n","        # nn 라이브러리를 활용해서 linear 모델을 선언해주자.\n","        # self.linear = ~~ 꼴로 작성해주자.\n","        # bert의 경우, 768차원 벡터를 산출해준다.\n","        # linear는 768차원의 값을 입력받아 7차원의 값을 출력해주어야 한다.\n","        ## 여기에 코드 작성\n","        self.drop = nn.Dropout(p=0.3)\n","\n","        self.linear = nn.Linear(768, num_labels)\n","\n","    def forward(self, input_ids, attention_mask):\n","        # 위에서 생성한 self.bert, self.linear 두 개의 layer를 활용하면 된다.\n","        # bert -> linear를 적용해준 다음, 출력값을 리턴해주자.\n","        # Hint: 만약 __init__에서 self.bert라는 bert layer를 생성하였고 이를 forward 함수에서 활용하려면, 생성한 bert layer의 forward 함수를 활용하면 될 것이다.\n","        #       bert layer의 forward 함수는 input_ids와 attention_mask를 변수로 입력받아야 한다. 이에 따라, self.bert(input_ids, attention_mask)와 같은 방식으로 코드를 작성하면 될 것이다.\n","        ## 여기에 코드 작성\n","        _,x = self.bert(input_ids, attention_mask)\n","        x = self.drop(x)\n","        x = self.linear(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GXFoQEm9-NUJ"},"outputs":[],"source":["# 분류를 위한 BERT 모델 생성\n","# BertModel을 초기화하는 역할\n","def BertModelInitialization():\n","    PATH = \"/content/gdrive/MyDrive/NLP/model.pt\"\n","    \n","    # BertModel은 다양한 작업을 진행할 수 있도록 해주는 여러 인터페이스들을 제공한다.\n","    # 그 중, 본 중간 미션의 task에 가장 적합한 인터페이스를 찾아보자.\n","    # Hint 1 : HuggingFace 홈페이지에 Bert를 검색해서 찾아보자.\n","    # URL : https://huggingface.co/docs/transformers/main/en/index\n","    # Hint 2 : 본 중간 미션이, 영화리뷰를 긍정과 부정의 두 가지 감정으로 분류해내는 작업임을 고려해보자.\n","    \n","    #model = BertClassifier()\n","    model = BertForSequenceClassification.from_pretrained('monologg/kobert', num_labels=7)\n","    # 생성한 모델을 특정 PATH에 저장\n","    torch.save(model.state_dict(), PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q8oypFVGdt2X"},"outputs":[],"source":["def get_model(device, cuda_available):\n","    PATH = \"model.pt\"\n","    \n","    #model = BertClassifier()\n","    model = BertForSequenceClassification.from_pretrained('monologg/kobert', num_labels=7)\n","\n","    if cuda_available:\n","        # PATH에 저장된 모델 불러옴\n","        model.load_state_dict(torch.load(PATH), strict=False)\n","\n","        # 불러온 모델을 device에 등록\n","        model = model.to(device)\n","    else:\n","        # PATH에 저장된 모델을 불러오기 및 불러온 모델을 device에 등록\n","        model.load_state_dict(torch.load(PATH, map_location=device), strict=False)\n","    \n","    return model"]},{"cell_type":"markdown","metadata":{"id":"fUf6MZ3zplat"},"source":["**신경망 성능 향상을 위한 다양한 툴**\n","\n","\n","\n","> 최종미션을 위한 이론문서를 확인해보자.\n","\n","\n","- 여러분은 옵티마이저, 에포크 수, 스케줄러 종류를 본 task에 가장 적합한 것으로 선택하여 바꿀 수 있다.\n","- 정답은 없다. 최적의 모델을 선택하여 신경망의 성능을 높여보자!\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BcFCBaNkXxrv"},"outputs":[],"source":["def get_model_with_params(num_data, device, cuda_available):\n","    model = get_model(device, cuda_available)\n","\n","    # 옵티마이저 설정\n","    optimizer = torch.optim.AdamW(model.parameters(),\n","                      lr = 5e-5, # 학습률\n","                      eps = 1e-8, # 0으로 나누는 것을 방지하기 위한 epsilon 값,\n","                    )\n","    # 전체 데이터가 총 몇 번 학습되는지\n","    epochs = 5\n","    warmup_ratio = 0.1\n","\n","\n","    # 총 훈련 스텝\n","    total_steps = num_data * epochs\n","    warmup_steps = int(total_steps* warmup_ratio)\n","    # 학습이 이루어짐에 따라 learning_rate을 감소시키기 위한 스케줄러\n","    scheduler = get_cosine_schedule_with_warmup(optimizer, \n","                                                num_warmup_steps = warmup_steps,\n","                                                num_training_steps = total_steps)\n","  \n","    # nn 라이브러리에서 crossentropy 형식으로 loss를 계산하는 모델을 찾아서 손실함수로 설정해주자.\n","    # criterion = ~~ 꼴로 작성해주자.\n","    ## 여기에 코드 작성\n","    # 손실 함수 정의\n","    criterion = nn.CrossEntropyLoss()\n","\n","    \n","    return model, optimizer, scheduler, epochs, criterion\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ihB4OGzxscad"},"outputs":[],"source":["def main():\n","    %cd /content/gdrive/MyDrive/NLP\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    BertModelInitialization()\n","    print(get_model_with_params(34388, device, torch.cuda.is_available()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KvhcU7f8Vdxy","colab":{"base_uri":"https://localhost:8080/"},"outputId":"537b9955-9ca6-4475-ed6b-0468351b16e2","executionInfo":{"status":"ok","timestamp":1660928770807,"user_tz":-540,"elapsed":8243,"user":{"displayName":"현시은","userId":"03817187635924431394"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/NLP\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:transformers.modeling_utils:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","WARNING:transformers.modeling_utils:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["(BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(8002, 768, padding_idx=1)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=7, bias=True)\n","), AdamW (\n","Parameter Group 0\n","    amsgrad: False\n","    betas: (0.9, 0.999)\n","    capturable: False\n","    eps: 1e-08\n","    foreach: None\n","    initial_lr: 5e-05\n","    lr: 0.0\n","    maximize: False\n","    weight_decay: 0.01\n","), <torch.optim.lr_scheduler.LambdaLR object at 0x7f187f578390>, 21, CrossEntropyLoss())\n"]}],"source":["if __name__ == '__main__':\n","      main()"]},{"cell_type":"code","source":[""],"metadata":{"id":"gCsCRREr0FEH"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"nlp_model_final.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}