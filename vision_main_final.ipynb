{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74aaf06b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "executionInfo": {
     "elapsed": 722,
     "status": "error",
     "timestamp": 1660548729315,
     "user": {
      "displayName": "오도열",
      "userId": "12861909783270182094"
     },
     "user_tz": -540
    },
    "id": "74aaf06b",
    "outputId": "08d085fb-5223-473b-e8b4-ff31eb81401e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from vision_model_final.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -mpy (c:\\users\\user\\anaconda3\\envs\\ml\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\user\\anaconda3\\envs\\ml\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\user\\anaconda3\\envs\\ml\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorboard (c:\\users\\user\\anaconda3\\envs\\ml\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -mpy (c:\\users\\user\\anaconda3\\envs\\ml\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\user\\anaconda3\\envs\\ml\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\user\\anaconda3\\envs\\ml\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorboard (c:\\users\\user\\anaconda3\\envs\\ml\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -mpy (c:\\users\\user\\anaconda3\\envs\\ml\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\user\\anaconda3\\envs\\ml\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\user\\anaconda3\\envs\\ml\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorboard (c:\\users\\user\\anaconda3\\envs\\ml\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -mpy (c:\\users\\user\\anaconda3\\envs\\ml\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\user\\anaconda3\\envs\\ml\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\user\\anaconda3\\envs\\ml\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorboard (c:\\users\\user\\anaconda3\\envs\\ml\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -mpy (c:\\users\\user\\anaconda3\\envs\\ml\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\user\\anaconda3\\envs\\ml\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\user\\anaconda3\\envs\\ml\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorboard (c:\\users\\user\\anaconda3\\envs\\ml\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -mpy (c:\\users\\user\\anaconda3\\envs\\ml\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\user\\anaconda3\\envs\\ml\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\user\\anaconda3\\envs\\ml\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorboard (c:\\users\\user\\anaconda3\\envs\\ml\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchsummary in c:\\users\\user\\anaconda3\\envs\\ml\\lib\\site-packages (1.5.1)\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "# 원하는 모델로 바꾸어 사용하기\n",
    "from vision_model_final import Mini_Xception,NN\n",
    "\n",
    "import cv2\n",
    "from PIL import ImageFont, ImageDraw, Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21c3f295",
   "metadata": {
    "id": "21c3f295"
   },
   "outputs": [],
   "source": [
    "# (주의) OpenCV는 Colab에서 제대로 실행되지 않음.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b6dbcc1",
   "metadata": {
    "id": "6b6dbcc1"
   },
   "outputs": [],
   "source": [
    "# 사용할 모델 선언하기\n",
    "NN = NN().to(device)\n",
    "\n",
    "# train이 아닌, evaluation 과정\n",
    "NN.eval()\n",
    "\n",
    "# 기존에 학습한 모델 불러오기. XXXXXXXXXXXX에 epoch 번호 작성.\n",
    "path = './checkpoint/model_weigths1_NN/weights_epoch_' + '144' + '.pth.tar'\n",
    "checkpoint = torch.load(path, map_location=device)\n",
    "NN.load_state_dict(checkpoint['mini_xception'])\n",
    "\n",
    "# Face detection을 위한 CascadeClassifier 모델 불러오기\n",
    "path = \"haarcascade_frontalface_default.xml\"\n",
    "face_detector = cv2.CascadeClassifier(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45f1f5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Face detection을 위한 DNN 모델\n",
    "# Abstract class / Interface\n",
    "class FaceDetectorIface:\n",
    "    def detect_faces(self, frame):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class DnnDetector(FaceDetectorIface):\n",
    "    \"\"\"\n",
    "        SSD (Single Shot Detectors) based face detection (ResNet-18 backbone(light feature extractor))\n",
    "    \"\"\"\n",
    "    def __init__(self, root=None):\n",
    "        self.prototxt = \"deploy.prototxt.txt\"\n",
    "        self.model_weights = \"res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "\n",
    "        if root:\n",
    "            self.prototxt = os.path.join(root, self.prototxt)\n",
    "            self.model_weights = os.path.join(root, self.model_weights)\n",
    "\n",
    "        self.detector = cv2.dnn.readNetFromCaffe(prototxt=self.prototxt, caffeModel=self.model_weights)\n",
    "        self.threshold = 0.5 # to remove weak detections\n",
    "\n",
    "    def detect_faces(self,frame):\n",
    "        h = frame.shape[0]\n",
    "        w = frame.shape[1]\n",
    "\n",
    "        # required preprocessing(mean & variance(scale) & size) to use the dnn model\n",
    "        \"\"\"\n",
    "            Problem of not detecting small faces if the image is big (720p or 1080p)\n",
    "            because we resize to 300,300 ... but if we use the original size it will detect right but so slow\n",
    "        \"\"\"\n",
    "        resized_frame = cv2.resize(frame, (300, 300))\n",
    "        blob = cv2.dnn.blobFromImage(resized_frame, 1.0, resized_frame.shape[0:2], (104.0, 177.0, 123.0))\n",
    "        # detect\n",
    "        self.detector.setInput(blob)\n",
    "        detections = self.detector.forward()\n",
    "        faces = []\n",
    "        # shape 2 is num of detections\n",
    "        for i in range(detections.shape[2]):\n",
    "            confidence = detections[0,0,i,2] \n",
    "            if confidence < self.threshold:\n",
    "                continue\n",
    "\n",
    "            # model output is percentage of bbox dims\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            box = box.astype(\"int\")\n",
    "            (x1,y1, x2,y2) = box\n",
    "\n",
    "            # x,y,w,h\n",
    "            faces.append((x1,y1,x2-x1,y2-y1))\n",
    "            # print(confidence)\n",
    "        return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b73d15e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detector_dnn = DnnDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c672bfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_vec(vec):\n",
    "    \"\"\"\n",
    "    7차원 감정 벡터의 순서를 일치시키기 위한 함수\n",
    "        \n",
    "    매개변수 (Parameters)\n",
    "    ----------\n",
    "    vec : numpy.ndarray, shape-(7,)\n",
    "        vision 모델의 출력값인 7차원 벡터.\n",
    "        fer2013 dataset의 emotion label의 순서대로 구한 감정 벡터. \n",
    "        \n",
    "    반환 값 (Returns)\n",
    "    -------\n",
    "    numpy.ndarray, shape-(7,)\n",
    "        NLP dataset의 emotion index의 순서대로 재정렬한 감정 벡터\n",
    "    \n",
    "    참고사항\n",
    "    -------\n",
    "    (fer2013 dataset)\n",
    "        0: 'Angry',\n",
    "        1: 'Disgust', \n",
    "        2: 'Fear', \n",
    "        3: 'Happy', \n",
    "        4: 'Sad', \n",
    "        5: 'Surprise', \n",
    "        6: 'Neutral'\n",
    "        \n",
    "    (NLP dataset)\n",
    "        0: 'Fear',\n",
    "        1: 'Surprise', \n",
    "        2: 'Angry', \n",
    "        3: 'Sad', \n",
    "        4: 'Neutral', \n",
    "        5: 'Happy', \n",
    "        6: 'Disgust'  \n",
    "    \"\"\"\n",
    "    ## 여기에 코드 작성\n",
    "    \n",
    "    return np.array([vec[2], vec[5], vec[0], vec[4], vec[6], vec[3], vec[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ac5936b",
   "metadata": {
    "id": "8ac5936b"
   },
   "outputs": [],
   "source": [
    "def get_label_emotion(label):\n",
    "    \"\"\"\n",
    "    label 값에 대응되는 감정의 이름 문자열을 구하기 위한 함수\n",
    "        \n",
    "    매개변수 (Parameters)\n",
    "    ----------\n",
    "    label : int\n",
    "        emotion label 번호\n",
    "        \n",
    "    반환 값 (Returns)\n",
    "    -------\n",
    "    String\n",
    "        label 번호에 대응되는 감정의 이름 문자열\n",
    "    \n",
    "    참고사항\n",
    "    -------\n",
    "    (NLP dataset)\n",
    "        0: 'Fear',\n",
    "        1: 'Surprise', \n",
    "        2: 'Angry', \n",
    "        3: 'Sad', \n",
    "        4: 'Neutral', \n",
    "        5: 'Happy', \n",
    "        6: 'Disgust'      \n",
    "    \"\"\"\n",
    "    ## 여기에 코드 작성\n",
    "\n",
    "    data = {0: 'Fear',\n",
    "            1: 'Surprise',\n",
    "            2: 'Angry',\n",
    "            3: 'Sad',\n",
    "            4: 'Neutral',\n",
    "            5: 'Happy',\n",
    "            6: 'Disgust'}\n",
    "    \n",
    "    return data[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88b87a6c",
   "metadata": {
    "id": "88b87a6c"
   },
   "outputs": [],
   "source": [
    "def cos_sim(A, B):\n",
    "    \"\"\"\n",
    "    두 벡터 A, B의 코사인 유사도를 구하기 위한 함수\n",
    "        \n",
    "    매개변수 (Parameters)\n",
    "    ----------\n",
    "    A : numpy.ndarray, shape-(N,)\n",
    "    B : numpy.ndarray, shape-(N,)\n",
    "        \n",
    "    반환 값 (Returns)\n",
    "    -------\n",
    "    numpy.float64\n",
    "        두 벡터 A, B의 코사인 유사도 값      \n",
    "    \"\"\"\n",
    "    ## 여기에 코드 작성\n",
    "    return np.dot(A, B) / (np.linalg.norm(A) * np.linalg.norm(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "296e40c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_video(nlp_vec, sentence):\n",
    "    \"\"\"\n",
    "    웹캠을 통해 받아온 실시간 영상 속에서\n",
    "    1) CascadeClassifier (혹은 다른 모델)을 통해 얼굴을 탐지하고\n",
    "    2) Mini_Xception (혹은 다른 모델)을 통해 얼굴 표정으로부터 7차원 감정 벡터를 추출하여\n",
    "    3) sort_vec 함수를 통해 2)에서 구한 감정 벡터의 순서를 재정렬한 후\n",
    "    4) cos_sim 함수를 이용하여 입력받은 문장에서 추출한 7차원 감정 벡터와의 코사인 유사도를 계산.\n",
    "    5) OpenCV 라이브러리를 사용하여, 문장과 표정에서 추출한 각 감정, 그리고 표정 연기에 대한 점수(코사인 유사도)를 window 상에 시각화.\n",
    "    \n",
    "    매개변수 (Parameters)\n",
    "    ----------\n",
    "    nlp_vec : 입력한 문장에서 추출한 7차원 감정 벡터\n",
    "    sentence : 사용자가 표정 연기 연습의 목적으로 입력한 문장\n",
    "    \"\"\"\n",
    "    \n",
    "    # OpenCV 실시간 웹캠 영상 불러오기\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # 프레임의 사이즈 계산 (height, width 구하기)\n",
    "    ## 여기에 코드 작성\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    \n",
    "    size = (width, height)\n",
    "    \n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_kor = ImageFont.truetype(\"malgun.ttf\", 18)\n",
    "    \n",
    "    softmax = torch.nn.Softmax()\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if ret:\n",
    "            # 문장의 최대 확률 감정\n",
    "            emotion_max = np.argmax(nlp_vec)\n",
    "            nlp_percentage = np.round(nlp_vec[emotion_max], 2)\n",
    "            nlp_emotion_label = get_label_emotion(emotion_max)\n",
    "            \n",
    "            # 한글 문장 출력을 위한 하단 색 띠 만들기\n",
    "            frame[height-70:height, 0:width] = (50, 100, 50)\n",
    "            frame_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "            \n",
    "            # 한글 문장 출력을 위한 PIL 라이브러리 사용\n",
    "            ## 여기에 코드 작성\n",
    "            draw = ImageDraw.Draw(frame_pil)\n",
    "            \n",
    "            text = \"emotion : \" + nlp_emotion_label + \" (\" + str(nlp_percentage) + \")\"\n",
    "            draw.text(((width // 2), height - 46),\n",
    "                      text,\n",
    "                      font=font_kor, fill=(255,255,255), anchor=\"mm\")\n",
    "            \n",
    "            text = \"\\\"\" + sentence + \"\\\"\"\n",
    "            draw.text(((width // 2), height - 23),\n",
    "                      text,\n",
    "                      font=font_kor, fill=(255,255,255), anchor=\"mm\")\n",
    "\n",
    "            # 한글 문장이 출력되어 있는 프레임, frame_GUI\n",
    "            frame_GUI = cv2.cvtColor(np.array(frame_pil), cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            # CascadeClassifier 이용한 얼굴 탐지 (faces : 얼굴 탐지 결과 얻어진, face(x,y,w,h)로 이루어진 sequential data)\n",
    "            ## 여기에 코드 작성\n",
    "            faces = face_detector_dnn.detect_faces(frame)\n",
    "            \n",
    "            for face in faces:\n",
    "                \n",
    "                (x,y,w,h) = face\n",
    "            \n",
    "                # 웹캠에서 인식한 얼굴을 모델에 넣어주기 위한 전처리\n",
    "                '''\n",
    "                전처리 후, input_face에 저장\n",
    "                 1) face의 좌표에 따라 얼굴 부분 프레임만 잘라내기\n",
    "                 2) BGR2GRAY로 흑백 변환하기\n",
    "                 3) (48,48)로 resize\n",
    "                 4) 히스토그램 평활화 적용\n",
    "                 5) Tensor로 바꾸고 device에 저장\n",
    "                 6) (1,48,48)로 차원 증가\n",
    "                '''\n",
    "                ## 여기에 코드 작성\n",
    "                face_frame = frame[y:y+h, x:x+w]\n",
    "                \n",
    "                face_frame = cv2.cvtColor(face_frame, cv2.COLOR_BGR2GRAY)\n",
    "                face_frame = cv2.resize(face_frame, (48, 48))\n",
    "                face_frame = cv2.equalizeHist(face_frame)\n",
    "                \n",
    "                face_frame = face_frame / 255\n",
    "                \n",
    "                input_face = torch.tensor(face_frame)\n",
    "                input_face = input_face.to(device)\n",
    "                input_face = input_face.reshape(1, 1, 48, 48)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # 모델 출력값의 shape : [1, 7, 1, 1]\n",
    "                    emotion_vec = NN(input_face.float()).squeeze()\n",
    "                    \n",
    "                    # 7차원 감정 확률 벡터\n",
    "                    vision_vec = softmax(emotion_vec)\n",
    "                    vision_vec = vision_vec.cpu().detach().numpy() # .reshape(-1,1)\n",
    "                    \n",
    "                    vision_vec = sort_vec(vision_vec)\n",
    "                    \n",
    "                    # 코사인 유사도 점수\n",
    "                    ## 여기에 코드 작성\n",
    "                    score = cos_sim(vision_vec, nlp_vec)\n",
    "\n",
    "                    # GUI 상에서 출력할 정보\n",
    "                    '''\n",
    "                     1) 한글 문장과 최대 확률 감정, 그 확률 (이미 PIL 라이브러리로 해결)\n",
    "                     2) 코사인 유사도 점수, score\n",
    "                     3) 표정의 최대 확률 감정과 그 확률\n",
    "                    '''\n",
    "                    ## 여기에 코드 작성\n",
    "                    vision_emotion_label = get_label_emotion(np.argmax(vision_vec))\n",
    "                    vision_percentage = np.max(vision_vec)\n",
    "                    \n",
    "                    # 얼굴 표정 주변의 정보 출력을 위한 OpenCV 라이브러리 사용\n",
    "                    ## 여기에 코드 작성\n",
    "                    cv2.rectangle(frame_GUI, (x,y), (x+w,y+h), (255,0,0), 2)\n",
    "                    cv2.rectangle(frame_GUI, (x,y), (x+w,y-70), (0,0,0), -1)\n",
    "                    cv2.putText(frame_GUI,\"Score: [{:2.1f}\"\"]\".format(score * 100),\n",
    "                                (x, y - 46),font\n",
    "                                ,0.6, (0,255,255))\n",
    "                    cv2.putText(frame_GUI,\n",
    "                                str(vision_emotion_label),\n",
    "                                (x, y - 23),font,\n",
    "                                0.4, (0, 255, 255))\n",
    "                    cv2.putText(frame_GUI,\n",
    "                                \"[{:.2f}]\".format(vision_percentage),\n",
    "                                (x + w // 2 + 10, y - 23), font,\n",
    "                                0.4, (255,255,0))\n",
    "                    \n",
    "                    \n",
    "            cv2.imshow(\"Video\", frame_GUI)\n",
    "            # 탈출 조건 : esc ( OxFF==27 )\n",
    "            if cv2.waitKey(1) & 0xff == 27:\n",
    "                break\n",
    "            \n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # 종료\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "vision_main_final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
